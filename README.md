# ü™Ñ *Fashion Try On and Animate* &nbsp;

<!-- Optional badges (example) -->
[ComfyUI](https://img.shields.io/badge/Made%20with-ComfyUI-blue?logo=data:image/svg+xml;base64,...)

---

## ‚ú® Overview

This workflow turns a single still photograph of a person + image of another garment + a control video into a fashion-forward animated clip of the person wearing the garment with movement controlled by the video. The garment identification is automated via Florence2.

The nodes for animation via Wan 2.1 are taken from here: [Wan 2.1 Fun Control Workflow](https://comfyui-wiki.com/en/tutorial/advanced/video/wan2.1/fun-control)

This workflow makes the combination of CatVTON and Wan2.1Control a lot more automated via Florence2 and SegmentAnything. There can be more improvements with better VLMs.

The driving videos were generated by Wan2.1 T2V 1.3B, except one but that could have been generated as well. Various other robust T2V options exist nowadays, and this removes the need to search manually for driving videos. Obviously, 1.3B is a relatively cheap model and the generation will have quality issues, but we only need the skeletal movement to be identified by the pose estimator.

## Demo

<div style="
  display: flex;
  flex-direction: column;
  row-gap: 4px;
  align-items: center;        /* center each row */
">

<div style="display:flex;gap:0px">
  <img src=https://drive.google.com/thumbnail?id=14bdy8rvFOUpuECcKYDdw3WU8dRyesmWa width=128 height=128 alt="Image 1_1">
  <img src=https://drive.google.com/thumbnail?id=1zR5l9ez45r-Xo8bKmbMRFmK7G3Z-KGq0 width=128 height=128 alt="Image 1_2">
  <img src=https://drive.google.com/thumbnail?id=1eWlZm86Dz_l42_uqnsvtBLCEORP-G0hf width=128 height=128 alt="Image 1_3">
  <img src=https://drive.google.com/thumbnail?id=1QnW2FwOuzdZ8Ip7K8jrMEQ6oUfcs2rnv width=128 height=128 alt="Image 1_4">
</div>

<div style="display:flex;gap:0px">
  <img src=https://drive.google.com/thumbnail?id=1r8WOHXE_Sd3KJPkeOlHh9fXoWSzTRVpx width=128 height=128 alt="Image 2_1">
  <img src=https://drive.google.com/thumbnail?id=1i3ZTkEbs9dcJz-dLqrulq06b2sETcgvn width=128 height=128 alt="Image 2_2">
  <img src=https://drive.google.com/thumbnail?id=1_MjlNAHDN2ynYIxcdj4_Aunh48GjrLms width=128 height=128 alt="Image 2_3">
  <img src=https://drive.google.com/thumbnail?id=1kYYEpBBFB6IwUIH2s2UKPjtxwQ-I17AV width=128 height=128 alt="Image 2_4">
</div>

<div style="display:flex;gap:0px">
  <img src=https://drive.google.com/thumbnail?id=1kC2aKWcGwRnbUSmrMsL-dR9Fc3Pt-G_U width=128 height=128 alt="Image 3_1">
  <img src=https://drive.google.com/thumbnail?id=1rjoKWZptZyVARO-apYoErkbjmEymXupa width=128 height=128 alt="Image 3_2">
  <img src=https://drive.google.com/thumbnail?id=1EBV1kxT_a6bgoQrUH6mDDMtBCxvEwR5Q width=128 height=128 alt="Image 3_3">
  <img src=https://drive.google.com/thumbnail?id=1BtDTYK2RSeRYCa8ir5Rl5tuOT20NEjkL width=128 height=128 alt="Image 3_4">
</div>
</div>

#### Stage 1: Virtual Garment Transfer with CatVTON

Inputs: Source person image (wearing any clothes) and the SKU image of the garment.

CatVTON is a very lightweight(even for mid-range laptop GPUs) virtual try-on diffusion network. warps the garment to match body pose and lighting, producing a re-clothed image while preserving the subject's identity. The identity preservation is fairly good, although this is sort of ruined in Stage 2 during animation.

At this point we have the re-clothed image which can be passed to Stage 2 for animation. 

#### Stage 2: Motion Conditioning with Wan 2.1 Control

Inputs: Video to control body movements + VTON image from Stage 1

The Wan 2.1 family of video diffusion models has many variants. Here we are using a variant that animates an input image given movement data from a input video(the movements are captured by a pose estimator).

The CatVTON output is the first frame providing a visual reference to flesh out the skeleton output from the pose estimator. The extracted pose sequence is the driver for motion across frames.

## Models used

Better versions of these models exist, such as 14B version of Wan 2.1 Control, F16 or F32 versions of CLIP text encoders, better CLIP vision encoders but they would not work well on my limited resources.

- CatVTON. Download from [here](https://drive.google.com/drive/folders/1TJNNql7UfDPVgHJuItDDjowycN5jpC5o). Put the downloaded folders into `ComfyUI/models/CatVTON`.

- Wan 2.1 Control + CLIP Encoders(Text and Vision). For the download links, check out [Wan 2.1 Fun Control Workflow](https://comfyui-wiki.com/en/tutorial/advanced/video/wan2.1/fun-control).

- gemma-3-4b-it-Q8_0.gguf[[Download link](https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/blob/main/gemma-3-4b-it-Q8_0.gguf)].

- Florence2 and the segmentation models should automatically download.

## üó∫Ô∏è Workflow
![Workflow Diagram](https://drive.google.com/thumbnail?id=1QqpipQ18ZKScNqodPybOaC0v5E12aE5P&sz=w1600)

Sample Data can be found [here](https://drive.google.com/drive/folders/1PgV-BMcbaVqJqkTdD4iOALoirxvOPpQB?usp=drive_link).

## Runtime
For 7-10 seconds of video, This workflow takes about 10 minutes on a Laptop RTX 4070(8 GB VRAM) with 32GB RAM. Stage 1 takes around 2 minutes, but the video generation takes longer.

## Limitations and Improvements

This workflow is mainly to explore a concept and not meant to be produtizable until and unless significant fine-tuning is done and the masking is improved.

- Use one decent local VLM instead of combining Florence2 + Gemma 3 4B. This would also be used to identify the prinicipal garment meant to be worn, because clear SKU images are not always available. Usually garments are already worn by models, so identifying the prinicipal garment from that image and segmenting it out is an important feature.

- Base model of CatVTON struggles with identity(both person and garment). It especially struggles with intricate patterns. Lot of ideas here. To start off, the model can be fine-tuned on larger higher-quality datasets(8-bit). Maybe training on 10-bit image data would push the model to learn better color preservation. 32-bit EXR might be possible in future as well, for certain rare professional use-cases. MSELoss is the one and only loss function used for training base model, maybe fine-tuning for a few epochs using LPIPS as well might result in better outputs.

- Using the 14B version of Wan2.1 Control would be better than the 1.3B one used here.

- Segmentation model + GroundingDINO are not upto the mark. Better models(maybe for example, FashionFormer ECCV 22) should exist, pre-trained ones trained on fashion datasets. One important improvement here to generate the mask based on the larger enveloping garment. This is not always the existing image on the model, it can be try-on garment as well.

- A combination of all above points could lead to training a model geared towards the Indian TAM, such as intricately patterned sarees, salwars, lehengas, etc.
